{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Prep ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data to train and test\n",
    "train_data = data[:33600, 1:].transpose() / 255\n",
    "train_labels = data[:33600 ,0]\n",
    "\n",
    "test_data = data[33601:, 1:].transpose()\n",
    "test_labels = data[33601:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(10,784) * 0.01\n",
    "b1 = np.random.randn(10, 1) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.matmul(w1, train_data) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 33600)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(matrix):\n",
    "    for i in range(0, matrix.shape[1]):\n",
    "        for j in range(0, 10):\n",
    "            if matrix[j,i] < 0:\n",
    "                matrix[j,i] = 0\n",
    "            \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Relu(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
       "           0.        ,    0.        ],\n",
       "       [1664.97589461,    0.        , 1023.51196007, ...,    0.        ,\n",
       "         905.42105208,  349.23207221],\n",
       "       [1764.16562412, 3270.88019555,    0.        , ..., 3533.6729674 ,\n",
       "        1832.45166582,    0.        ],\n",
       "       ...,\n",
       "       [1186.29238091, 3514.72689112,    0.        , ...,    0.        ,\n",
       "        1535.31065586, 3331.28267939],\n",
       "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
       "           0.        ,    0.        ],\n",
       "       [1294.27118489,    0.        ,    0.        , ...,    0.        ,\n",
       "           0.        ,    0.        ]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = np.random.randn(10,10) * 0.01\n",
    "b2 = np.random.randn(10, 1) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = np.matmul(w2, a1) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability improvement\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = Softmax(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 33600)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 33600)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Rewritten in Class format (Locked In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(matrix):\n",
    "    return np.maximum(matrix, 0)\n",
    "\n",
    "def Deriv_Relu(matrix):\n",
    "    return matrix > 0\n",
    "\n",
    "def Softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability improvement\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self, a0, learning_rate=0.001):\n",
    "        # Weights and Biases\n",
    "        self.w1 = np.random.rand(10, 784) - 0.5\n",
    "        self.b1 = np.random.rand(10, 1)   - 0.5\n",
    "        self.w2 = np.random.rand(10, 10)  - 0.5\n",
    "        self.b2 = np.random.rand(10, 1)   - 0.5\n",
    "        \n",
    "        # Others\n",
    "        self.a0 = a0\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward_pass(self):\n",
    "        # Hidden Layer\n",
    "        self.z1 = np.matmul(self.w1, self.a0) + self.b1\n",
    "        self.a1 = Relu(self.z1)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.z2 = np.matmul(self.w2, self.a1) + self.b2\n",
    "        self.a2 = Softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def cost_function(self, labels):\n",
    "        # one-hot encode labels\n",
    "        self.y = np.zeros((labels.max() + 1, labels.size))\n",
    "        self.y[labels, np.arange(labels.size)] = 1\n",
    "        \n",
    "        # Cost Function (Mean Square Error)\n",
    "        self.cost = np.sum(np.power((self.a2 - self.y), 2) / 2) / 784\n",
    "        \n",
    "        return self.cost\n",
    "    \n",
    "    def backward_pass_not_working(self):\n",
    "        dA2dZ2 = self.a2 - (1 - self.a2)\n",
    "        dCdA2 = np.sum(self.a2 - self.y) / 784\n",
    "        dZ2dW2 = self.a1.transpose()\n",
    "        \n",
    "        self.dW2 = dCdA2 * np.matmul(dA2dZ2, dZ2dW2)\n",
    "        \n",
    "        # dCdA2\n",
    "        # dA2dZ2\n",
    "        dZ2dA1 = self.w2.transpose()\n",
    "        dA1dZ1 = Deriv_Relu(self.z1)\n",
    "        dZ1dW1 = self.a0.transpose()\n",
    "        \n",
    "        dZ2 = dCdA2 * dA2dZ2\n",
    "        \n",
    "        # We element wise multiply the derivative of Relu as Relu is an element-wise activation function\n",
    "        self.dW1 = np.matmul(np.matmul(dZ2dA1, dZ2)*dA1dZ1,  dZ1dW1)\n",
    "        \n",
    "        return self.dW1, self.dW2 \n",
    "    \n",
    "    def backward_pass(self):\n",
    "        m = 784\n",
    "        self.dZ2 = self.a2 - self.y\n",
    "        self.dW2 = 1 / m * self.dZ2.dot(self.a1.T)\n",
    "        self.db2 = 1 / m * np.sum(self.dZ2)\n",
    "        self.dZ1 = self.w2.T.dot(self.dZ2) * Deriv_Relu(self.z1)\n",
    "        self.dW1 = 1 / m * self.dZ1.dot(self.a0.T)\n",
    "        self.db1 = 1 / m * np.sum(self.dZ1)\n",
    "        \n",
    "        return self.dW1, self.dW2\n",
    "    \n",
    "    def grad_descent(self):\n",
    "        self.w1 -= self.learning_rate * self.dW1\n",
    "        self.w2 -= self.learning_rate * self.dW2\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        self.b2 -= self.learning_rate * self.db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "\n",
    "    for i in range(0, 33599):\n",
    "        if x.a2[:, i].argmax() == train_labels[i]:\n",
    "            correct += 1\n",
    "        else: \n",
    "            wrong +=1\n",
    "            \n",
    "    accuracy = (correct / (correct + wrong))*100\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2701488 , -0.06333839, -0.41303122, ..., -0.47049951,\n",
       "        -0.44952697,  0.25424486],\n",
       "       [-0.44181561, -0.16244422, -0.43954549, ..., -0.12471968,\n",
       "         0.17562699, -0.2807511 ],\n",
       "       [ 0.35673168,  0.38327482,  0.16177345, ...,  0.21417211,\n",
       "         0.33143282,  0.34229856],\n",
       "       ...,\n",
       "       [-0.11740341, -0.17045203,  0.1993784 , ...,  0.42613486,\n",
       "        -0.14474687,  0.06434644],\n",
       "       [ 0.38887865, -0.48480583, -0.3653902 , ..., -0.18851569,\n",
       "         0.11801506,  0.22301514],\n",
       "       [ 0.41471847,  0.07892538, -0.41540931, ...,  0.2259472 ,\n",
       "        -0.26256731, -0.34797288]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 21.426621928586865\n",
      "0 accuracy: 14.220661329206226\n",
      "\n",
      "1 cost: 21.426571772250043\n",
      "1 accuracy: 14.652221792315249\n",
      "\n",
      "2 cost: 21.42652031055428\n",
      "2 accuracy: 15.101639929759813\n",
      "\n",
      "3 cost: 21.42646766479353\n",
      "3 accuracy: 15.670109229441353\n",
      "\n",
      "4 cost: 21.42641439814233\n",
      "4 accuracy: 16.018333878984496\n",
      "\n",
      "5 cost: 21.426361646162807\n",
      "5 accuracy: 16.387392481919104\n",
      "\n",
      "6 cost: 21.42631082400291\n",
      "6 accuracy: 16.756451084853715\n",
      "\n",
      "7 cost: 21.42626408057248\n",
      "7 accuracy: 17.006458525551356\n",
      "\n",
      "8 cost: 21.426224552503406\n",
      "8 accuracy: 17.205869222298283\n",
      "\n",
      "9 cost: 21.426195682871246\n",
      "9 accuracy: 17.28027619869639\n",
      "\n",
      "10 cost: 21.42618122255148\n",
      "10 accuracy: 17.17908271079496\n",
      "\n",
      "11 cost: 21.42618541086467\n",
      "11 accuracy: 17.1344385249561\n",
      "\n",
      "12 cost: 21.426211620892552\n",
      "12 accuracy: 17.19098782701866\n",
      "\n",
      "13 cost: 21.426260076486745\n",
      "13 accuracy: 17.1225334087324\n",
      "\n",
      "14 cost: 21.426331052427532\n",
      "14 accuracy: 17.012411083663203\n",
      "\n",
      "15 cost: 21.42642670123846\n",
      "15 accuracy: 16.98264829310396\n",
      "\n",
      "16 cost: 21.426570608526113\n",
      "16 accuracy: 17.074912943837614\n",
      "\n",
      "17 cost: 21.42680597940738\n",
      "17 accuracy: 17.07788922289354\n",
      "\n",
      "18 cost: 21.42715658317505\n",
      "18 accuracy: 17.28622875680824\n",
      "\n",
      "19 cost: 21.42758289751832\n",
      "19 accuracy: 17.5392124765618\n",
      "\n",
      "20 cost: 21.428093432401347\n",
      "20 accuracy: 17.673145034078395\n",
      "\n",
      "21 cost: 21.428694306093124\n",
      "21 accuracy: 17.354683175094497\n",
      "\n",
      "22 cost: 21.429264207150304\n",
      "22 accuracy: 16.417155272478347\n",
      "\n",
      "23 cost: 21.42997639102678\n",
      "23 accuracy: 15.036161790529482\n",
      "\n",
      "24 cost: 21.43095201367118\n",
      "24 accuracy: 13.616476680853598\n",
      "\n",
      "25 cost: 21.431916936486218\n",
      "25 accuracy: 12.607518080895264\n",
      "\n",
      "26 cost: 21.432584725423904\n",
      "26 accuracy: 11.931902735200453\n",
      "\n",
      "27 cost: 21.43294672391598\n",
      "27 accuracy: 11.524152504538826\n",
      "\n",
      "28 cost: 21.433171464941374\n",
      "28 accuracy: 11.327718086847822\n",
      "\n",
      "29 cost: 21.433320478630744\n",
      "29 accuracy: 11.26223994761749\n",
      "\n",
      "30 cost: 21.43339530774028\n",
      "30 accuracy: 11.238429715170094\n",
      "\n",
      "31 cost: 21.43339943669778\n",
      "31 accuracy: 11.229500878002321\n",
      "\n",
      "32 cost: 21.43336160511675\n",
      "32 accuracy: 11.229500878002321\n",
      "\n",
      "33 cost: 21.433352437888818\n",
      "33 accuracy: 11.229500878002321\n",
      "\n",
      "34 cost: 21.43344019791442\n",
      "34 accuracy: 11.229500878002321\n",
      "\n",
      "35 cost: 21.433568090782817\n",
      "35 accuracy: 11.229500878002321\n",
      "\n",
      "36 cost: 21.4336430167003\n",
      "36 accuracy: 11.229500878002321\n",
      "\n",
      "37 cost: 21.433667435552326\n",
      "37 accuracy: 11.229500878002321\n",
      "\n",
      "38 cost: 21.433672636810027\n",
      "38 accuracy: 11.229500878002321\n",
      "\n",
      "39 cost: 21.433673392421245\n",
      "39 accuracy: 11.229500878002321\n",
      "\n",
      "40 cost: 21.433673465220433\n",
      "40 accuracy: 11.232477157058247\n",
      "\n",
      "41 cost: 21.433673469263457\n",
      "41 accuracy: 11.232477157058247\n",
      "\n",
      "42 cost: 21.433673469385774\n",
      "42 accuracy: 11.229500878002321\n",
      "\n",
      "43 cost: 21.433673469387738\n",
      "43 accuracy: 6.9198488050239595\n",
      "\n",
      "44 cost: 21.433673469387756\n",
      "44 accuracy: 3.0506860323223908\n",
      "\n",
      "45 cost: 21.433673469387756\n",
      "45 accuracy: 5.503139974404\n",
      "\n",
      "46 cost: 21.433673469387756\n",
      "46 accuracy: 7.854400428584183\n",
      "\n",
      "47 cost: 21.433673469387756\n",
      "47 accuracy: 9.065746004345367\n",
      "\n",
      "48 cost: 21.433673469387756\n",
      "48 accuracy: 9.580642281020268\n",
      "\n",
      "49 cost: 21.433673469387756\n",
      "49 accuracy: 9.759219024375726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Neural_Network(train_data)\n",
    "\n",
    "for i in range(50):\n",
    "    x.forward_pass()\n",
    "    cost = x.cost_function(train_labels)\n",
    "    x.backward_pass()\n",
    "    x.grad_descent()\n",
    "    \n",
    "    # np.savetxt(f\"w2-{i}.csv\", x.w2, delimiter=\",\")\n",
    "    \n",
    "    print(f\"{i} cost: {cost}\")\n",
    "    print(f\"{i} accuracy: {get_accuracy()}\")\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
